# DMD + RL Training Configuration
# This configuration enables combined Distribution Matching Distillation with Reinforcement Learning
#
# Key Features:
# - DMD loss for distribution matching
# - RL loss from video reward model (DifferentiableVideoReward) with direct backprop
# - Frame sampling to reduce reward model VRAM (e.g. 84 frames -> 10 frames)
# - Gradient checkpointing on reward model + chunked VAE decode
# - LoRA support for stable training
# - Cold start mechanism for RL
#
# Usage:
#   torchrun --nproc_per_node=8 train.py --config configs/self_forcing_dmd_rl.yaml

# Generator checkpoint (pre-trained)
generator_ckpt: checkpoints/self_forcing_dmd.pt


# FSDP wrap strategies
generator_fsdp_wrap_strategy: size
real_score_fsdp_wrap_strategy: size
fake_score_fsdp_wrap_strategy: size
text_encoder_fsdp_wrap_strategy: size

# CPU offload switches (FSDP)
generator_cpu_offload: true
real_score_cpu_offload: true
fake_score_cpu_offload: true
text_encoder_cpu_offload: true
rl_reward_cpu_offload: true

# Model names
real_name: Wan2.1-T2V-14B

# Denoising configuration
denoising_step_list:
- 1000
- 750
- 500
- 250
warp_denoising_step: true
ts_schedule: false
num_train_timestep: 1000
timestep_shift: 5.0
guidance_scale: 3.0
denoising_loss_type: flow

# Precision and seed
mixed_precision: true
seed: 0

# Wandb logging (replace with your credentials)
wandb_host: WANDB_HOST
wandb_key: WANDB_KEY
wandb_entity: WANDB_ENTITY
wandb_project: WANDB_PROJECT

# Distributed training
sharding_strategy: hybrid_full

# Optimizer settings
lr: 2.0e-06
lr_critic: 4.0e-07
beta1: 0.0
beta2: 0.999
beta1_critic: 0.0
beta2_critic: 0.999
weight_decay: 0.01

# Data
data_path: prompts/vidprom_filtered_extended.txt
batch_size: 1

# EMA
ema_weight: 0.99
ema_start_step: 200
log_iters: 200

# Negative prompt for CFG
negative_prompt: '色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走'

# Training ratio
dfake_gen_update_ratio: 5

# Video shape [B, T, C, H, W]
image_or_video_shape:
- 1
- 21
- 16
- 60
- 104

# ============================================
# DMD + RL specific configuration
# ============================================
distribution_loss: dmd_rl
trainer: score_distillation

# RL loss configuration
rl_loss_weight: 0.005              # Weight for RL loss: loss = dmd_loss + rl_loss_weight * rl_loss
rl_cold_start_steps: 5000         # Number of steps before RL loss is enabled (cold start)
rl_reward_checkpoint: VideoAlign/checkpoints  # Path to reward model checkpoint
rl_target_height: 336            # Target height for reward model (must be multiple of 28)
rl_target_width: 504             # Target width for reward model (must be multiple of 28)
rl_reward_type: overall          # Reward type: 'VQ', 'MQ', 'TA', or 'overall'

# Reward normalization
# Uses inference_config from reward model checkpoint (VQ_mean/std, MQ_mean/std, TA_mean/std)
# for z-score normalization. Loaded automatically from model_config.json.
rl_reward_ema_decay: 0.99        # EMA decay for reward monitoring statistics (logging only)

# Memory optimization for RL
rl_reward_num_frames: 8         # Number of frames to sample for reward model (0 = all frames)
                                 # 21 latent frames -> 84 pixel frames -> sample 10 for reward
                                 # Reduces Qwen2-VL visual tokens by ~8x, attention memory by ~64x
rl_frame_strategy: full_decode_uniform  # full_decode_uniform | sparse_window_tail
rl_sparse_num_groups: 4         # For sparse_window_tail: remaining latent frames are split into 4 groups
rl_sparse_window_back: 4        # For sparse_window_tail: decode [i-4, i] and keep window tail frame
rl_reward_fsdp: true            # Set to true to FSDP-shard the reward model across GPUs
vae_chunk_size: 1                # Temporal chunk size for VAE decode (0 = no chunking, full checkpoint)
                                 # Reward model gradient checkpointing is always enabled (via Qwen2VL API)

# ============================================
# LoRA configuration (optional, for stable training)
# ============================================
use_lora: true                  # Set to true to enable LoRA
lora_rank: 16                    # Rank of LoRA matrices
lora_alpha: 32                   # Alpha parameter for LoRA scaling
lora_dropout: 0.0                # Dropout probability for LoRA layers
# lora_target_modules:           # Uncomment to specify custom target modules
#   - q_proj
#   - k_proj
#   - v_proj
#   - o_proj

# Other settings
gradient_checkpointing: true
num_frame_per_block: 3
load_raw_video: false
no_visualize: true
no_save: false
gc_interval: 50
output_dir: output                   # Base directory for experiment logs

# Experiment naming and training control
exp_name: rl_lora_test          # Experiment name for folder naming (replaces config name)
max_training_steps: 1000            # Maximum training steps (null for unlimited)
num_demo_videos: 20                  # Number of demo videos to generate after training (0 to disable)

# Self-forcing specific
num_training_frames: 21
independent_first_frame: false
same_step_across_blocks: true
backward_simulation: true
last_step_only: false
context_noise: 0.0
i2v: false

# Model kwargs
model_kwargs:
  timestep_shift: 5.0

# Gradient clipping
max_grad_norm_generator: 10.0
max_grad_norm_critic: 10.0


                                 # Sub-sampling happens AFTER VAE decode to preserve temporal coherence
                                 # All latent frames are decoded in contiguous chunks (no latent sub-sampling)
                                 # Each chunk is gradient-checkpointed; smaller = less peak VAE memory
                                 # Example: 21 latent frames, chunk_size=5 → 5 chunks decoded sequentially
